{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_gen = ImageDataGenerator(rotation_range=30, #rotate the image 30 degrees\n",
    "                              width_shift_range=0.1, #shift the pic width by a max of 10%\n",
    "                              height_shift_range = 0.1, #shift the pic height by a max of 10%\n",
    "                              rescale = 1/255, #rescale the image by normalizing it\n",
    "                              shear_range = 0.2, #Shear means cutting away part of the image(max 20%)\n",
    "                              zoom_range=0.2, #Zoom in by a max of 20%\n",
    "                               horizontal_flip=True, #allow horizontal flipping\n",
    "                               fill_mode=\"nearest\" #Fill in the missing pixels with the nearest filled value\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_shape=(100,100,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense, Conv2D, MaxPooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters = 32, kernel_size=(3,3), input_shape = image_shape, activation=\"relu\",))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(filters = 64, kernel_size=(3,3), input_shape = image_shape, activation=\"relu\",))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(filters = 64, kernel_size=(3,3), input_shape = image_shape, activation=\"relu\",))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 98, 98, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 49, 49, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 47, 47, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 23, 23, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 21, 21, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 10, 10, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 6400)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               819328    \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 129       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 875,777\n",
      "Trainable params: 875,777\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 982 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "train_image_gen = image_gen.flow_from_directory('E:/AI Projects/Image Classification/Dataset/Train',\n",
    "                                               target_size=image_shape[:2],\n",
    "                                               batch_size=batch_size,\n",
    "                                               class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 330 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_image_gen = image_gen.flow_from_directory('E:/AI Projects/Image Classification/Dataset/Test',\n",
    "                                               target_size=image_shape[:2],\n",
    "                                               batch_size=batch_size,\n",
    "                                               class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Apple Braeburn': 0, 'Banana': 1}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_image_gen.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "250/250 [==============================] - 11s 46ms/step - loss: 1.2602e-05 - accuracy: 1.0000 - val_loss: 1.7524e-06 - val_accuracy: 1.0000\n",
      "Epoch 2/5\n",
      "250/250 [==============================] - 11s 46ms/step - loss: 3.4960e-06 - accuracy: 1.0000 - val_loss: 3.6666e-09 - val_accuracy: 1.0000\n",
      "Epoch 3/5\n",
      "250/250 [==============================] - 11s 44ms/step - loss: 3.3925e-06 - accuracy: 1.0000 - val_loss: 3.3830e-10 - val_accuracy: 1.0000\n",
      "Epoch 4/5\n",
      "250/250 [==============================] - 12s 46ms/step - loss: 1.0440e-06 - accuracy: 1.0000 - val_loss: 5.8400e-10 - val_accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "250/250 [==============================] - 11s 44ms/step - loss: 1.2781e-06 - accuracy: 1.0000 - val_loss: 1.0029e-11 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "results = model.fit_generator(train_image_gen, epochs = 5,\n",
    "                             steps_per_epoch = 250,\n",
    "                             validation_data = test_image_gen,\n",
    "                             validation_steps = 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x20f850845c0>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(results.history['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('fruit_classifier.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
